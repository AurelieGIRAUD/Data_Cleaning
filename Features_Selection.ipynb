{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asrqI3A6A-V7"
      },
      "source": [
        "# FEATURES SELECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides functions to apply various method of features selection such as:\n",
        "- Pearson correlation\n",
        "- Chi-squared\n",
        "- Recursive Feature Elimination\n",
        "- Lasso\n",
        "- Random Forest"
      ],
      "metadata": {
        "id": "k24q_wbgIsIc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBT9Bn-0A-V_"
      },
      "source": [
        "## Filter Based"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FszFE_GdA-V_"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "    \n",
        "Correlation\" measures used should depend on the type of variables being investigated:\n",
        "\n",
        "- continuous variable v continuous variable: use \"traditional\" correlation - e.g. Spearman's rank correlation or Pearson's linear correlation.\n",
        "- continuous variable v categorical variable: use an ANOVA F-test / difference of means\n",
        "- categorical variable v categorical variable: use Chi-square / Cramer's \n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2JhOKAkA-WA"
      },
      "source": [
        "### Pearson correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLAHZ58UA-WA"
      },
      "outputs": [],
      "source": [
        "# ONLY FOR CONTINOUS-CONTINOUS VARIABLES\n",
        "# We want to keep only features not correlated so we check the correlations between a target columns and features columns\n",
        "\n",
        "def correlation_selector(X, y):\n",
        "    target = X.loc[:,y]\n",
        "    test = X.loc[:, X.columns != y]\n",
        "    cor_list = []\n",
        "    feature_name = X.columns.tolist()\n",
        "    \n",
        "# calculate the correlation with y for each feature\n",
        "    for i in test.columns.tolist():\n",
        "        cor = st.pearsonr(X[i], target)[0]\n",
        "        pvalue = st.pearsonr(X[i], target)[1]\n",
        "        cor_list.append(cor)\n",
        "    \n",
        "    if pvalue <= 0.05:\n",
        "        if cor >= 0.75 or cor <= -0.75:\n",
        "            cor_list.append(cor)\n",
        "            \n",
        "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-len(X):]].columns.tolist() # get the feature's names\n",
        "    cor_support = [True if i in cor_feature else False for i in feature_name] # Feature selection\n",
        "  \n",
        "    \n",
        "    return cor_support, cor_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCr7_wGjA-WC"
      },
      "outputs": [],
      "source": [
        "# We check the absolute value of the Pearsonâ€™s correlation between the target and numerical features in our dataset. \n",
        "# We keep the top n features based on this criterion.\n",
        "\n",
        "def cor_selector(X, y,num_feats):\n",
        "    cor_list = []\n",
        "    feature_name = X.columns.tolist()\n",
        "    # calculate the correlation with y for each feature\n",
        "    for i in X.columns.tolist():\n",
        "        cor = np.corrcoef(X[i], y)[0, 1]\n",
        "        cor_list.append(cor)\n",
        "    # replace NaN with 0\n",
        "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
        "    # feature name\n",
        "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n",
        "    # feature selection? 0 for not select, 1 for select\n",
        "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
        "    return cor_support, cor_feature\n",
        "cor_support, cor_feature = cor_selector(X, y,num_feats)\n",
        "print(str(len(cor_feature)), 'selected features')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKwytTrtA-WD"
      },
      "source": [
        "### Chi-Squared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHhN6---A-WE"
      },
      "outputs": [],
      "source": [
        "# we calculate the chi-square metric between the target and the numerical variable.\n",
        "# We only select the variable with the maximum chi-squared values.\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "X_norm = MinMaxScaler().fit_transform(X)\n",
        "chi_selector = SelectKBest(chi2, k=num_feats)\n",
        "chi_selector.fit(X_norm, y)\n",
        "chi_support = chi_selector.get_support()\n",
        "chi_feature = X.loc[:,chi_support].columns.tolist()\n",
        "print(str(len(chi_feature)), 'selected features')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjklqNn-A-WE"
      },
      "source": [
        "## Wrapped based"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_3gKWHPA-WF"
      },
      "source": [
        "### Recursive Feature Elimination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuLDjdSNA-WF"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "- The goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. \n",
        "- First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. \n",
        "- Then, the least important features are pruned from current set of features. \n",
        "- That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RV-Vj6iIA-WG"
      },
      "outputs": [],
      "source": [
        "# we could use any estimator with the method. In this case, we use LogisticRegression\n",
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "rfe_selector = RFE(estimator=LogisticRegression(), n_features_to_select=num_feats, step=10, verbose=5)\n",
        "rfe_selector.fit(X_norm, y)\n",
        "rfe_support = rfe_selector.get_support()\n",
        "rfe_feature = X.loc[:,rfe_support].columns.tolist()\n",
        "print(str(len(rfe_feature)), 'selected features')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKpbPZhNA-WG"
      },
      "source": [
        "## Embedded based"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGymuR5SA-WG"
      },
      "source": [
        "### Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuQdn4iNA-WG"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "embeded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l1\"), max_features=num_feats)\n",
        "embeded_lr_selector.fit(X_norm, y)\n",
        "\n",
        "embeded_lr_support = embeded_lr_selector.get_support()\n",
        "embeded_lr_feature = X.loc[:,embeded_lr_support].columns.tolist()\n",
        "print(str(len(embeded_lr_feature)), 'selected features')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IG7X89QA-WH"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUhwOdSQA-WH"
      },
      "outputs": [],
      "source": [
        "# We calculate feature importance using node impurities in each decision tree. \n",
        "# The final feature importance is the average of all decision tree feature importance.\n",
        "\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=num_feats)\n",
        "embeded_rf_selector.fit(X, y)\n",
        "\n",
        "embeded_rf_support = embeded_rf_selector.get_support()\n",
        "embeded_rf_feature = X.loc[:,embeded_rf_support].columns.tolist()\n",
        "print(str(len(embeded_rf_feature)), 'selected features')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQsNlbSeA-WH"
      },
      "source": [
        "### All"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXW0XJ0yA-WI"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "# put all selection together\n",
        "feature_selection_df = pd.DataFrame({'Feature':feature_name, 'Pearson':cor_support, 'Chi-2':chi_support, 'RFE':rfe_support, 'Logistics':embeded_lr_support,\n",
        "                                    'Random Forest':embeded_rf_support, 'LightGBM':embeded_lgb_support})\n",
        "# count the selected times for each feature\n",
        "feature_selection_df['Total'] = np.sum(feature_selection_df, axis=1)\n",
        "# display the top 100\n",
        "feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n",
        "feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
        "feature_selection_df.head(num_feats)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc-autonumbering": true,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}